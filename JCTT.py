# main.py
import os
import streamlit as st
import cohere
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.embeddings.base import Embeddings

# --- 1. ì´ˆê¸° ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ ---
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒìˆ˜ ì •ì˜
COHERE_API_KEY = os.getenv("r1Fl17yD8nqp8yoYtnpiGKZXPMadYECdMJHZ1hCo")
PDF_FILES = ['H1J.pdf', 'H2J.pdf', 'H3J.pdf']
PERSIST_DIRECTORY = "chroma_db"
COLLECTION_NAME = "school_bot"

# --- 2. Cohere Embeddings í´ë˜ìŠ¤ ì •ì˜ ---
class CohereEmbeddings(Embeddings):
    """Cohere APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ LangChain Embedding ë˜í¼ í´ë˜ìŠ¤"""
    def __init__(self, client, model="embed-multilingual-v3.0"):
        self.client = client
        self.model = model

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        response = self.client.embed(
            texts=texts, model=self.model, input_type="search_document"
        )
        return response.embeddings

    def embed_query(self, text: str) -> list[float]:
        response = self.client.embed(
            texts=[text], model=self.model, input_type="search_query"
        )
        return response.embeddings[0]

# --- 3. í•µì‹¬ ë¡œì§ í•¨ìˆ˜í™” ---
@st.cache_resource
def load_vector_store():
    """PDFë¥¼ ë¡œë“œí•˜ê³  Vector Storeë¥¼ ìƒì„± ë˜ëŠ” ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (Streamlit ìºì‹± í™œìš©)"""
    if not COHERE_API_KEY:
        st.error("Cohere API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    
    cohere_client = cohere.Client(api_key=COHERE_API_KEY)
    embeddings = CohereEmbeddings(client=cohere_client)

    # Vector DBê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if os.path.exists(PERSIST_DIRECTORY) and os.listdir(PERSIST_DIRECTORY):
        st.info("ê¸°ì¡´ Vector DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
        return Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME
        )

    # Vector DBê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    st.info("ìƒˆë¡œìš´ Vector DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    documents = []
    for file in PDF_FILES:
        if os.path.exists(file):
            loader = PyPDFLoader(file)
            documents.extend(loader.load())
        else:
            st.warning(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file}")

    if not documents:
        st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. PDF íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()

    text_splitter = CharacterTextSplitter(chunk_size=700, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)
    
    vector_store = Chroma.from_documents(
        texts,
        embedding=embeddings,
        persist_directory=PERSIST_DIRECTORY,
        collection_name=COLLECTION_NAME
    )
    st.success("Vector DB ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return vector_store

def generate_response(retriever, user_question: str) -> str:
    """RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    docs = retriever.get_relevant_documents(user_question)
    context = "\n\n".join([doc.page_content for doc in docs])

    prompt = f"""Use the following pieces of context to answer the users question shortly.
Given the following summaries of a long document and a question, create a final answer with references ("SOURCES"), use "SOURCES" in capital letters regardless of the number of sources.
If you don't know the answer, just say that "I don't know", don't try to make up an answer.
If you need it, look it up on the internet.
You MUST answer in Korean and in Markdown format.

----------------
{context}

ì§ˆë¬¸: {user_question}
ë‹µë³€:"""
    
    cohere_client = cohere.Client(api_key=COHERE_API_KEY)
    response = cohere_client.chat(message=prompt)
    return response.text

# --- 4. Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="PDF ê¸°ë°˜ í•™êµ ì „ìš© ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– í•™êµ ì „ìš© ì±—ë´‡ (ì „ê³µì‹¬í™”íƒêµ¬)")

try:
    vector_store = load_vector_store()
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! í•™êµì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”. (í•™ì‚¬ì¼ì • ë“±)"
        }]

    for msg in st.session_state["messages"]:
        st.chat_message(msg["role"]).write(msg["content"])

    if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
        st.session_state["messages"].append({"role": "user", "content": user_input})
        st.chat_message("user").write(user_input)

        answer = generate_response(retriever, user_input)
        st.session_state["messages"].append({"role": "assistant", "content": answer})
        st.chat_message("assistant").write(answer)

except Exception as e:
    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
