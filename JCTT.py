import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings.base import Embeddings
import cohere
#ìœ„ì— ë§í–ˆë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
cohere_client = cohere.Client(api_key="r1Fl17yD8nqp8yoYtnpiGKZXPMadYECdMJHZ1hCo") #apií‚¤ ë¶ˆëŸ¬ì˜¤ê¸°

class CohereEmbeddings(Embeddings): #LangChainì—ì„œ ì“¸ Cohere ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜í•˜ê¸°
    def __init__(self, client, model="embed-multilingual-v3.0"): # í´ë¼ì´ì–¸íŠ¸ë‘ ëª¨ë¸ ì´ë¦„ ì§€ì • ì—¬ê¸°ì„  embed-multilingual -v3.0 ëª¨ë¸ ì§€ì •
        self.client = client
        self.model = model

    def embed_documents(self, texts): #ê¸€->ë²¡í„° ë³€í™˜ í•¨ìˆ˜
        input_texts = [t if isinstance(t, str) else t.page_content for t in texts] #ë¬¸ì„œê°€ ê°ì²´ì¼ ê²½ìš° í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œë‹¤.
	#Cohere APIë¥¼ ì¨ì„œ ì„ë² ë”© ì‹¤í–‰
        response = self.client.embed(
            texts=input_texts,
            model=self.model,
            input_type="search_document"
        )
        return response.embeddings #ë°˜í™˜

    def embed_query(self, text):#ì§ˆë¬¸->ë²¡í„°ë¡œ ë³€í™˜ í•¨ìˆ˜
	#Cohere APIë¥¼ ì¨ì„œ ì„ë² ë”© ì‹¤í–‰
        response = self.client.embed(
            texts=[text],
            model=self.model,
            input_type="search_query"
        )
        return response.embeddings[0]#ë°˜í™˜
#ì£¼ìš” ê¸°ëŠ¥
#pdf ë¬¸ì„œ ì§‘ì–´ë„£ê¸°
pdf_files = ['H1J.pdf','H2J.pdf', 'H3J.pdf'] #pdf íŒŒì¼ ëª©ë¡ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
documents = [] #ì €ì¥í•  ì „ì²´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
for file in pdf_files: #ìœ„ì—ì„œ ì •ì˜í•œ pdf_filesì„ fileì— ë„£ìœ¼ë©´ì„œ ë°˜ë³µ
    loader = PyPDFLoader(file)#PyPDFLoaderë¥¼ ì¨ì„œ íŒŒì¼ì„ ì½ìŒ
    documents.extend(loader.load()) #pdf íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ì½ì–´ì„œ LangChainì˜ document ê°ì²´ í˜•íƒœë¡œ ì €ì¥ë˜ëŠ” ì½”ë“œì„

#í…ìŠ¤íŠ¸ ë¶„í• 
#AIë¥¼ ìœ„í•œê²ƒ
text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=100)#ë¬¸ì„œë¥¼ 700ì ë‹¨ìœ„ë¡œ ë‚˜ëˆ”, ì²­í¬ì— ê²¹ì¹˜ëŠ” ë¬¸ì 200ì ì„¤ì •í•´ì„œ ì§¤ë¦¬ëŠ” ìƒí™© ì•ˆë§Œë“¤ê²Œ í•¨
texts = text_splitter.split_documents(documents) #ë‚˜ëˆˆ ì²­í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥

#ë²¡í„° ì €ì¥
embeddings = CohereEmbeddings(client=cohere_client)#ìœ„ì—ì„œ ì •ì˜í•´ë†¨ë˜ Cohere ì„ë² ë”© ë§Œë“¤ê¸°
vector_store = Chroma.from_documents(texts, embedding=embeddings)#ë¬¸ì„œ ì„ë² ë”©í•˜ê³  Chromaì— ì €ì¥ -> ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •í•¨
retriever = vector_store.as_retriever(search_kwargs={"k": 3}) #ê°€ì¥ ì—°ê´€ì„±ì´ ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ë ¤ê³  ì“°ëŠ” ì½”ë“œ / kê°’ = ìœ ì‚¬ë„ë¼ê³  ìƒê°(ì›ë˜ ë¬¸ì„œ ê°œìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ”ë° 3ìœ¼ë¡œ ì„¤ì •í•˜ë‹ˆê¹Œ ìê¾¸ ë¬¸ì„œë¥¼ ëª»ì°¾ì•„ì„œ ì„ì˜ë¡œ 5ë¡œ ì„¤ì •í•¨

def cohere_chat_generate(prompt: str) -> str:#Cohere í•¨ìˆ˜ ì •ì˜í•˜ê¸°
    response = cohere_client.chat(message=prompt)#í”„ë¡¬í”„íŠ¸(ë’¤ì— ì •ì˜ë¨)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ë§Œë“¤ê¸°
    return response.text#ì‘ë‹µ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
#streamlit ì£¼ìš” UI êµ¬ì„±í•˜ê¸°
st.set_page_config(page_title="PDF ê¸°ë°˜ í•™êµ ì „ìš© ì±—ë´‡", page_icon="ğŸ¤–", layout="wide") #ì œëª©,ì•„ì´ì½˜,í™”ë©´ ë„“ì´ ì„¤ì •
st.title("ğŸ¤– í•™êµ ì „ìš© ì±—ë´‡ ( ì „ê³µì‹¬í™”íƒêµ¬ )")#í˜ì´ì§€ì—ì„œ í‘œì‹œí•  ì£¼ ì œëª©

if "messages" not in st.session_state:#ë§Œì•½ì— ì „ì— ëŒ€í™”í–ˆë˜ê²Œ ì—†ìœ¼ë©´ ì´ˆê¸° ë©”ì‹œì§€ ì¶œë ¥í•˜ê¸°
    st.session_state["messages"] = [{
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! í•™êµ ìƒí™œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”. (í•™ì‚¬ì¼ì •/êµìœ¡ê³¼ì • í¸ì„±í‘œ í•™ìŠµ ì™„ë£Œ)\n\n**ì˜ˆì‹œ ì§ˆë¬¸:**\n- 1í•™ê¸° 2ì°¨ê³ ì‚¬ ì‹œí—˜ê¸°ê°„ ì•Œë ¤ì¤˜\n- 3í•™ë…„ ì§„ë¡œê³¼ëª©ì—ëŠ” ë­ê°€ ìˆì–´?\n- ì¸ê³µì§€ëŠ¥ ê¸°ì´ˆ ê³¼ëª©ì˜ ì„±ì·¨ë„ ë¶„í•  ë°©ì‹ì€?"
    }]
#ì•ì—ì„œ ë§Œë“  ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ê¸° ( í•œë²ˆì¨° ë¬»ê¸° -> ê·¸ê±°ì— ëŒ€ë‹µ, ë‘ ë²ˆì§¸ ë¬»ê¸° -> ê·¸ê±°ì— ëŒ€ë‹µí•˜ëŠ”ë° ì•ì— í–ˆë˜ ëŒ€í™” ìœ ì§€í•˜ê³  ëŒ€ë‹µí•˜ê¸° ê¸°ëŠ¥)
for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])
#ë‹µë³€ ë§Œë“œëŠ” í•¨ìˆ˜ ì •ì˜
def generate_response(user_question: str) -> str:
    docs = retriever.get_relevant_documents(user_question) #ì§ˆë¬¸ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
    context = "\n\n".join([doc.page_content for doc in docs]) #pdfë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë§Œë“¤ì–´ì„œ ì €ì¥

    prompt = f"""You are an AI assistant for a school. Answer the user's question based ONLY on the provided context below.
Your main goal is to provide accurate information based on the documents.
If the answer is not available in the context, you MUST say "ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
Do not try to make up an answer. Answer in Korean and in Markdown format.
ë‹µë³€ì„ í•  ë•Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ë„ ë¬´ì¡°ê±´ í•œë²ˆ ë” ìƒê°í•´.
í•œë²ˆ ë” ìƒê° í•  ë•Œ ì œëŒ€ë¡œ ìƒê°í•´.
ê°œí•™ì‹ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ í•œë²ˆ ë” ìƒê°í•˜ê³  2í•™ê¸° ê°œí•™ì‹ì€ 8ì›” 13ì¼ì´ì•¼.
ê·¸ë¦¬ê³  í•™ì‚¬ì¼ì •ì„ ì§ˆë¬¸ë°›ì•˜ì„ ë•Œ ê°œí•™ì‹ë„ 2í•™ê¸°ì—” 8ì›” 13ì¼ì´ì•¼.

CONTEXT:
{context}

----------------

ì§ˆë¬¸: {user_question}

ë‹µë³€:
""" # {context}ì™€ {user_q~}ëŠ” ì•ì—ì„œ ë§Œë“  ë³€ìˆ˜ë¥¼ í”„ë¡¬í¬íŠ¸ì— ë„£ì€ê²ƒ, í”„ë¡¬í”„íŠ¸ ë‚´ì—ì„œ SOURCESë¥¼ ì‚¬ìš©í•´ì„œ ì°¸ê³  ë¬¸í—Œ í‘œì‹œí•˜ê¸°, 

    return cohere_chat_generate(prompt) #ë³´ì´ëŠ”ê±°ë‘ ë˜‘ê°™ì´ Cohereì—ì„œ ë‹µë³€ ë§Œë“¤ê¸°

#ë§ˆì§€ë§‰ ì½”ë“œ
#ì‚¬ìš©ì ì§ˆë¬¸ -> ì‹¤í–‰ë˜ëŠ” ì½”ë“œ
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state["messages"].append({"role": "user", "content": user_input})#ì…ë ¥ê°’ì„ ì €ì¥í•¨
    st.chat_message("user").write(user_input)#í™”ë©´ì— ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë©”ì‹œì§€ í‘œì‹œ

    answer = generate_response(user_input)#pdfë¥¼ ê¸°ë°˜í•œ ë‹µë³€ ë§Œë“¤ê¸°
    st.session_state["messages"].append({"role": "assistant", "content": answer})#Cohereê°€ ë§Œë“  ë‹µë³€ì„ ì €ì¥í•¨
    st.chat_message("assistant").write(answer)#ë‹µë³€ í™”ë©´ì— ì¶œë ¥í•˜ê¸°
